# 왜 scaling을 해야 할까?
* 조건수 : 작은 변화의 비율에 대해 함수가 얼마나 변화할것인가?
* 조건수가 크면 약간의 오차만 있어도 전혀 다른 값이 된다.

( 내 생각 읽기 전에 역전파 복습!! https://wooono.tistory.com/216)

## 내 생각....
* 기본 퍼셉트론에서....
* 변수 a,b,c,d 가 있고 예측값 p와 실제값 gt가 있다.
* a와 gt는 가격이므로 값이 매우 크고 b,c,d는 매우 작다.
* p=sigmoid(aw0+bw1+cw2+dw3)는 aw0때문에 매우 크게 나올것이고 p와 gt는 크기가 대략 비슷할 것이다.
* 역전파시 mae -> 부호차이, mse -> b,c,d에게는 꽤 큰 값
* sigmoid(1-sigmoid)는 aw0+bw1+cw2+dw3가 매우 크므로 언제나 매우 매우 작은값
* 역전파시 mae 기준 = lr * (부호 * 매우 작은 값 * w_n)   (n은 0,1,2,3)
* 결국에는 w0 변화 시키는 것이 의미가 있고 나머지는 거의 의미가 없어질 것이다.

#### 조건수가 커지는 경우
* 변수들의 단위 차이로 인해 숫자의 스케일이 달라지는 경우
* 다중 공선성 발생 -> 변수 선택, PCA 사용

#### StandardScaler
* N(0,1)로 만듬
* 이상치가 있을 경우 균형잡힌 척도 만들 수 X

```python
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()
print(standardScaler.fit(train_data))
train_data_standardScaled = standardScaler.transform(train_data)
```

#### MinMaxScaler
* 0~1로 데이터 조정
* 이상치가 있을 경우 변환된 값이 매우 좁을 수 있음

```python
from sklearn.preprocessing import MinMaxScaler
minMaxScaler = MinMaxScaler()
print(minMaxScaler.fit(train_data))
train_data_minMaxScaled = minMaxScaler.transform(train_data)
```


#### MaxAbsScaler
* 절대값이 0~1사이로 매핑 
* 이상치에 민감

```python
from sklearn.preprocessing import MaxAbsScaler
maxAbsScaler = MaxAbsScaler()
print(maxAbsScaler.fit(train_data))
train_data_maxAbsScaled = maxAbsScaler.transform(train_data)
```

#### RobustScaler
* IQR 범위 내에서 데이터 스케일링
* 중앙값을 기준으로 스케일링



```python
from sklearn.preprocessing import RobustScaler
robustScaler = RobustScaler()
print(robustScaler.fit(train_data))
train_data_robustScaled = robustScaler.transform(train_data)
```


